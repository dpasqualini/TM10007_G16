{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "# %pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U scikit-learn\n",
    "# %pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extentions used in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from worcliver.load_data import load_data, load_Ft_set, load_D_set, second_split, load_Tr_set, load_Va_set\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import importlib\n",
    "import function_files.self_made\n",
    "\n",
    "importlib.reload(function_files.self_made)\n",
    "\n",
    "from function_files.self_made import classifiers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used in assignment\n",
    "There are multiple functions used for this assignment. Every function is explained in the module of the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_X_y(df):\n",
    "    '''\n",
    "    Returns dataframe y_d including the labels, and a seperate dataframe X_d including the feature information.\n",
    "    '''\n",
    "    y_d = df.pop('label').to_frame()\n",
    "    X_d = df\n",
    "\n",
    "    return X_d, y_d\n",
    "\n",
    "\n",
    "def preprocessing(feature_df):\n",
    "    '''\n",
    "    This function preprocesses the features by removing zero-variance, zero-filled,\n",
    "    and nan-filled features from the dataset.\n",
    "    '''\n",
    "\n",
    "    filter = VarianceThreshold(threshold=0.0)\n",
    "    df_filter1 = filter.fit_transform(feature_df)\n",
    "    features_kept = filter.get_support()\n",
    "    names = feature_df.columns[features_kept]\n",
    "    df_filter2 = pd.DataFrame(df_filter1, columns=names)\n",
    "    zero_var = pd.DataFrame(feature_df.columns[~features_kept], columns=[\"removed_features\"])\n",
    "    zero_var_removed = feature_df.columns[~features_kept].tolist()\n",
    "\n",
    "    # remove zero-filled features (if >80% is zero)\n",
    "    zero_percents = (df_filter2 == 0).sum() / len(df_filter2) * 100\n",
    "    threshold = 80\n",
    "    nonzero_cols = df_filter2.columns[zero_percents <= threshold]\n",
    "    zero_cols = df_filter2.columns[zero_percents > threshold]\n",
    "    df_filtered = pd.DataFrame(df_filter2, columns=nonzero_cols)\n",
    "    high_zero_frac_removed = zero_cols.tolist()\n",
    "\n",
    "    print(f\"Number of removed columns due to zero-variance: {len(zero_var)}\")\n",
    "    print(f\"Number of removed non-zero-variance columns due to fraction zero > 80%: {len(zero_cols)}\")\n",
    "    print(f'Remaining number of features after preprocessing: {df_filtered.shape[1]}')\n",
    "\n",
    "    removed_features = zero_var_removed + high_zero_frac_removed\n",
    "\n",
    "    return df_filtered, removed_features\n",
    "\n",
    "\n",
    "def outliers(X_filtered):\n",
    "    means = X_filtered.mean()\n",
    "    stds = X_filtered.std()\n",
    "    threshold = 3\n",
    "    for feature in X_filtered.columns:\n",
    "\n",
    "        # Calculate the lower and upper bounds for the feature\n",
    "        lower_bound = means[feature] - (stds[feature] * threshold)\n",
    "        upper_bound = means[feature] + (stds[feature] * threshold)\n",
    "\n",
    "        # Replace outliers with upper and lower bound\n",
    "        X_filtered.loc[X_filtered[feature] < lower_bound, feature] = lower_bound\n",
    "        X_filtered.loc[X_filtered[feature] > upper_bound, feature] = upper_bound\n",
    "    \n",
    "    return X_filtered\n",
    "\n",
    "\n",
    "def normalize_standard_v1(column):\n",
    "    '''\n",
    "    This function will normalize a panda Series using the standard scaling.\n",
    "    It will return a new column with normalized numbers with a mean of 0 and a standard deviation of 1. \n",
    "    Assumed is that the data has a normal distribution. To apply this to a dataframe\n",
    "    this function needs to be applied to each column (which can be done with the use of .apply())\n",
    "    '''\n",
    "    feat2 = column.to_numpy()\n",
    "    standard_scale = StandardScaler()\n",
    "    feat_scaled2 = standard_scale.fit_transform(feat2.reshape(-1, 1))\n",
    "    return pd.Series(feat_scaled2.flatten(), index=column.index)\n",
    "\n",
    "\n",
    "def plt_tsne(df_features, labels):\n",
    "    '''\n",
    "    Using this function will return a 2 dimensional t-SNE plot.\n",
    "    May be used as visualisation and possible feature extraction.\n",
    "    '''\n",
    "\n",
    "    labels = labels['label'].replace({'benign': 0, 'malignant': 1})\n",
    "    labels = labels.to_numpy()\n",
    "    df_features = df_features.to_numpy()\n",
    "\n",
    "    # Perform TSNE\n",
    "    tsne = TSNE(n_components=2, learning_rate=\"auto\", perplexity=5)\n",
    "    X_tsne = tsne.fit_transform(df_features, labels)\n",
    "\n",
    "    # Plot the t-SNE representation colored by the labels\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels)\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def plt_lda(df_features, labels):\n",
    "    '''\n",
    "    Using this function will return a 1 dimensional LDA plot. May be used as feature extraction.\n",
    "    '''\n",
    "\n",
    "    labels = labels['label'].replace({'benign': 0, 'malignant': 1})\n",
    "    labels = labels.to_numpy()\n",
    "    df_features = df_features.to_numpy()\n",
    "\n",
    "    # Perform LDA\n",
    "    lda = LDA()\n",
    "    X_lda = lda.fit_transform(df_features, labels)\n",
    "\n",
    "    # Plot the t-SNE representation colored by the labels\n",
    "    plt.scatter(X_lda, np.zeros_like(X_lda), c=labels)\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def PCA_info_notransform(X, selected_features):\n",
    "    # Covariance matrix\n",
    "    covar_matrix = PCA(n_components=len(selected_features))\n",
    "    covar_matrix.fit(X)\n",
    "\n",
    "    variance = covar_matrix.explained_variance_ratio_\n",
    "    var = np.cumsum(np.round(variance, decimals=3)*100)\n",
    "\n",
    "    return variance, var\n",
    "\n",
    "\n",
    "def PCA_plot(var):\n",
    "    plt.ylabel('% Variance Explained')\n",
    "    plt.xlabel('# of Features')\n",
    "    plt.title('PCA Variance Explained')\n",
    "    plt.ylim(min(var), 100.5)\n",
    "    plt.style.context('seaborn-whitegrid')\n",
    "    plt.axhline(y=90, color='r', linestyle='--')\n",
    "    plt.plot(var)\n",
    "    plt.show()\n",
    "\n",
    "    for j, val in enumerate(var):\n",
    "        if val > 90:\n",
    "            print(\"The index of the first number that is bigger than 90% is:\", j)\n",
    "            n_comp = j+1\n",
    "            break\n",
    "        \n",
    "    return n_comp\n",
    "\n",
    "\n",
    "def PCA_transform(X_tr_normalized, n_comp):\n",
    "    pca = PCA(n_components=n_comp)  # as index starts at 0 the number of components is one higher\n",
    "    X_pca = pca.fit_transform(X_tr_normalized)\n",
    "\n",
    "    variance = pca.explained_variance_ratio_\n",
    "    var = np.cumsum(np.round(variance, decimals=3)*100)\n",
    "\n",
    "    return pca, X_pca, variance, var\n",
    "\n",
    "\n",
    "def PCA_transform_overview(variance, var):\n",
    "    n_components = len(variance)\n",
    "    idx = np.arange(n_components)+1\n",
    "\n",
    "    df_explained_variance = pd.DataFrame([variance, var],\n",
    "                                        index=['explained variance', 'cumulative [%]'],\n",
    "                                        columns=idx).T\n",
    "\n",
    "    mean_explained_variance = df_explained_variance.iloc[:, 0].mean()  # calculate mean explained variance\n",
    "\n",
    "    # DISPLAY info about PCs\n",
    "    print('PCA Overview')\n",
    "    print('='*40)\n",
    "    print(\"Total: {} components\".format(n_components))\n",
    "    print('-'*40)\n",
    "    print('Mean explained variance:', round(mean_explained_variance, 3))\n",
    "    print('-'*40)\n",
    "    print(df_explained_variance)\n",
    "    print('-'*40)\n",
    "\n",
    "    return df_explained_variance, mean_explained_variance, idx\n",
    "\n",
    "\n",
    "def scree_plot(df_explained_variance, mean_explained_variance, idx):\n",
    "    limit_df = 15  # change this number if you want to have more or less PCA components visualized\n",
    "    df_explained_variance_limited = df_explained_variance.iloc[:limit_df, :]\n",
    "\n",
    "    # make scree plot\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "    ax1.set_title('Explained variance across principal components', fontsize=14)\n",
    "    ax1.set_xlabel('Principal component', fontsize=12)\n",
    "    ax1.set_ylabel('Explained variance', fontsize=12)\n",
    "\n",
    "    ax2 = sns.barplot(x=idx[:limit_df], y='explained variance', data=df_explained_variance_limited, palette='summer')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.grid(False)\n",
    "\n",
    "    ax2.set_ylabel('Cumulative', fontsize=14)\n",
    "    ax2 = sns.lineplot(x=idx[:limit_df]-1, y='cumulative [%]', data=df_explained_variance_limited, color='#fc8d59')\n",
    "\n",
    "    ax1.axhline(mean_explained_variance, ls='--', color='#fc8d59')  # plot mean\n",
    "    ax1.text(-.8, mean_explained_variance+(mean_explained_variance*.05), \"average\",\n",
    "             color='#fc8d59', fontsize=14)  # label y axis\n",
    "\n",
    "    max_y1 = max(df_explained_variance_limited.iloc[:, 0])\n",
    "    max_y2 = max(df_explained_variance_limited.iloc[:, 1])\n",
    "    ax1.set(ylim=(0, max_y1+max_y1*.1))\n",
    "    ax2.set(ylim=(0, max_y2+max_y2*.1))\n",
    "\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def factor_loadings(X, pca, df_explained_variance):\n",
    "    # DEFINE PARAMETERS\n",
    "    selected_features = X.columns\n",
    "    top_k = 3\n",
    "\n",
    "    # PCA factor loadings\n",
    "    df_c = pd.DataFrame(pca.components_, columns=selected_features).T\n",
    "\n",
    "    print(\"Factor Loadings for the 1. component \\n(explains {0:.2f} of the variance)\"\n",
    "          .format(df_explained_variance.iloc[0, 0]))\n",
    "    print('='*40, '\\n')\n",
    "    print('Top {} highest'.format(top_k))\n",
    "    print('-'*40)\n",
    "    print(df_c.iloc[:, 0].sort_values(ascending=False)[:top_k], '\\n')\n",
    "\n",
    "    print('Top {} lowest'.format(top_k))\n",
    "    print('-'*40)\n",
    "    print(df_c.iloc[:, 0].sort_values()[:top_k])\n",
    "\n",
    "    # Plot heatmap\n",
    "    size_yaxis = round(X[selected_features].shape[1] * 0.5)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, size_yaxis))\n",
    "    sns.heatmap(df_c.iloc[:, :], annot=True, cmap=\"YlGnBu\", ax=ax)\n",
    "\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def classifiers(clsfs, X_pca, y_train):\n",
    "    f1_scores = []\n",
    "    # Loop over each classifier and fit the model, make predictions and compute metrics\n",
    "    for clf in clsfs:\n",
    "        clf.fit(X_pca, y_train)\n",
    "        y_pred = clf.predict(X_pca)\n",
    "        accuracy = metrics.accuracy_score(y_train, y_pred)\n",
    "        F1 = metrics.f1_score(y_train, y_pred)\n",
    "        f1_scores.append(F1)\n",
    "        precision = metrics.precision_score(y_train, y_pred)\n",
    "        recall = metrics.recall_score(y_train, y_pred)\n",
    "        print(type(clf).__name__)\n",
    "        print('Accuracy:', accuracy)\n",
    "        print('F1:', F1)\n",
    "        print('Precision:', precision)\n",
    "        print('Recall:', recall)\n",
    "        print()\n",
    "\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean-up (before splitting data)\n",
    "In this section double entries, such as repeated features and samples, will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These 2 lines add a duplicate sample in the DataFrame, you can than observe what the effect is \n",
    "in the printed sentence. If you'd like to see it indeed works, please uncomment these lines.\n",
    "'''\n",
    "# row = df.iloc[0].copy()\n",
    "# df = df.append(row, ignore_index=True)\n",
    "'''\n",
    "In the lines below we first drop all duplicate entries, whether it is a duplicate row or a column. \n",
    "Additionally, the number of duplicates will be returned to the user.\n",
    "'''\n",
    "\n",
    "df_f = df.drop_duplicates()\n",
    "print(f'The number of samples (after removal of duplicates): {len(df_f.index)}, the number of duplicates that had to be removed: {len(df.index)-len(df_f.index)}')\n",
    "print(f'The number of columns (after removal of duplicates): {len(df_f.columns)}, the number of duplicates that had to be removed: {len(df.columns)-len(df_f.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_f                                       # So we can just use df from now on\n",
    "df = df.replace('', np.nan)                     # Any empty values are replaced for NaN\n",
    "df.describe(include='all')                      # Uncomment if interested\n",
    "\n",
    "check_for_nan = df.isnull().values.any()    # Checking whether the dataset contains of NaN\n",
    "print(f'Are there NaN existing in the resting dataset?: {check_for_nan}')\n",
    "\n",
    "non_numeric_columns = df.select_dtypes(exclude='number').columns.tolist()  # Label is a non_numeric_column\n",
    "numeric_columns = df.select_dtypes(include='number').columns.tolist()\n",
    "print(f'amount of numeric features = {len(numeric_columns)}')  # All features are numeric_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The table above shows there are only 2 labels present in the dataset, from observation of the csv.file we can see the labels are 'malignant' and 'benigne'. What is also stated here, that 94 of the 186 samples was labeled malignant (and thus 92 samples are labeled benigne.) This information is usefull in the next step, to determine whether a stratified splitting of our data, would be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "In this section the dataset will be split into two datasets, the Design set (D_set) and the Final Test set (Ft_set). This splitting will be performed randomly where 80% will end up in D_set and the remaining 20% will be stored in Ft_set. Ft_set will not be used at all, until the very latest to evaluate our tool and establish how generalising the tool is. In the initial split we will randomly split the dataset, as both labels are equally abundant in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_set, Ft_set = train_test_split(df, test_size=0.2)\n",
    "# D_set = pd.DataFrame(D_set)\n",
    "# Ft_set = pd.DataFrame(Ft_set)                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following lines were used to save the splitted sets, to make sure Design and Test set won't alter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_set.to_csv(\"C:/Users/dgjpa/D_set.csv\")\n",
    "# Ft_set = Ft_set.to_csv(\"C:/Users/dgjpa/Ft_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lines below, we try to gain some insight in what information our Design dataset contains and how it looks like as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_s = load_D_set()\n",
    "D_s.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_s.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_s.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have splitted the Design set into a dataframe including the label, and a dataframe including only the information without label. It can be observed that we have a total of 148 samples in our Design set with a total of 493 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d, y_d = return_X_y(D_s)\n",
    "X_d.shape, y_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting (2/2)\n",
    "First, there will be two functions who return dataframes from the save .csv file. D_usable and Ft_usable are both DataFrames including both the labels as the feature information, and are the resulting datasets from the initial split as can be seen above.\n",
    "\n",
    "X_d is a Dataframe including all the information about a case but with the labels removed, whereas y_d is a DataFrame including only the labels.\n",
    "\n",
    "Also a second split is performed to ensure our preprocessing steps can be validated. The second split should result in a total of 44 files. A total of 11 pairs of validation (ca. 10%) and train sets (ca. 90%), which are consequently splitted into a .csv file containing only feature information (X_Tr/Va) and csv-files containing only the label per sample (y_Tr/Va)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split Design set into train and validation sets.\n",
    "# second_split(X_d, y_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Now that we have splitted our data, we can continue with the preprocessing steps on our training set. It was previously observed there were some zero-variance features present in the Design set. These zero-variances will not contribute in differentiating between any of the sampels and therefore can be savely removed. However, non-zero but small variances are chosen not to be removed based only on their variance as even minor variances could have significant impact in the differentiation between maligne and benigne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will remove the columns with zero-variance and will return the names of the removed columns, the columns from this Dataframe will be used to remove the same columns in subsequent steps in our model. In other words, based on the zero-variance in specific columns in our training set, these features will not be considered in the validation and test sets as well.\n",
    "\n",
    "Additionally, the non-zero-variance columns will be screened for fraction of zero or NaN entries. If a column consists of more than 80% of zero's, the column will be removed. Also the names of the specific columns will be returned and used to remove the corresponding columns from validation and test sets as well. \n",
    "\n",
    "An overview will be made of the features which still have zero's in them. The count of the zero's will be done and the mean with its std and the mean without the zeros with its std is shown of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr1, y_tr1 = load_Tr_set(1)\n",
    "# X_tr1.shape, y_tr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered, removed_features_preprocess = preprocessing(X_d) # X_filtered, dataframe containing only columns with a non-zero variance. Features_removed, dataframe containing the names of the removed features.\n",
    "\n",
    "# zero_count = (X_filtered == 0.0).sum(axis=0)  # Makes a list of the features containing zeros and counts them. \n",
    "# features_zero = pd.concat([zero_count, X_filtered.mean(), X_filtered.std(), X_filtered[X_filtered !=0].mean(), X_filtered[X_filtered !=0].std()], axis=1)  # The mean and std will be shown of each feature that contains zero. These are calculated with and without the zeros.\n",
    "# features_zero.columns = ['zero_count', 'mean', 'std', 'mean_without_0', 'std_without_0']  # Added the names of the columns\n",
    "# features_zero = features_zero[features_zero['zero_count'] > 0]  # Only shows features that contains one or multiple zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus a total of 30 features could be removed during the preprocessing step!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handeling of Outliers\n",
    "An outlier was identified as such if an entry was more than three std's separated from the mean of the corresponding feature. The outlier was then replaced with the value of the corresponding lower or upper bound. This method was chosen to preserve the overall distribution of the data as best as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handeling of outliers\n",
    "X_filtered = outliers(X_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "To give every feature the same weight, all features should be scaled. We choose to do this by min-max scaling. \n",
    "Below you find the code to apply this normalization to each column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = X_filtered.apply(normalize_standard_v1)\n",
    "\n",
    "# # Create a box plot of the original data\n",
    "# X_filtered.boxplot(figsize=(10,5))\n",
    "# plt.title('Original Data')\n",
    "# plt.show()\n",
    "\n",
    "# # Create a box plot of the normalized data by standard scaling\n",
    "# X_tr_norm2.boxplot(figsize=(10,5))\n",
    "# plt.title('Normalized Data Standard')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use our data we need to transform the labels, and as we have only two labels this is easy and we chose to code benign as 0 and malignant as 1. Additionally we will have to transform both the X and the y Dataframes to a numpy array, in order to use scikit learn modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_tsne(X_norm, y_d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code beneath was not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_lda(X_norm, y_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "n_original = X_norm.shape[1]\n",
    "feature_sizes = [10, 40, 115, n_original]\n",
    "le = LabelEncoder()\n",
    "y_d_bin = le.fit_transform(y_d.to_numpy().reshape(-1,))\n",
    "\n",
    "# classifiers = [LDA(), RandomForestClassifier(), SVC(probability=True)]\n",
    "# titles = ['LDA', 'RF', 'SVC']\n",
    "\n",
    "# for i, clf in enumerate(classifiers):\n",
    "#     plt.subplot(1, 3, i+1)\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('ROC Curve {}'.format(titles[i]))\n",
    "#     for size in feature_sizes:\n",
    "#         model = LogisticRegression(max_iter=1000)\n",
    "#         rfe = RFE(estimator=model, n_features_to_select=size)\n",
    "#         rfe.fit(X_tr_norm, y_d.to_numpy().reshape(-1,))\n",
    "#         selected_features = X_tr_norm.columns[rfe.support_]\n",
    "#         X_tr_norm_selected = X_tr_norm.loc[:, selected_features]\n",
    "#         clf.fit(X_tr_norm_selected, y_d.to_numpy().reshape(-1,))\n",
    "#         y_score_selected = clf.predict_proba(X_tr_norm_selected)\n",
    "#         fpr_selected,tpr_selected,thesholds_selected = roc_curve(y_d_bin, y_score_selected[:, 1])\n",
    "#         n_selected = X_tr_norm_selected.shape[1]\n",
    "#         plt.plot(fpr_selected, tpr_selected, label=f'Selected {n_selected} from {n_original} features.')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(estimator=model, n_features_to_select=120)\n",
    "rfe.fit(X_norm, y_d.to_numpy().reshape(-1,))\n",
    "final_features = X_norm.columns[rfe.support_]\n",
    "final_X_norm = X_norm.loc[:, final_features]\n",
    "\n",
    "final_X_norm.boxplot(figsize=(10,5))\n",
    "plt.title('Normalised data scaled by standard scaling after feature selection by RFE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction \n",
    "Principal Component Analisys (PCA) is used to reduce the dimensions of this complex problem. It uses the current features to recreate new \"features\" also called components. These components are each constructed with a combination of the current features.\n",
    "\n",
    "To determine how many components will be created, it is necessary to get some insight in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get info about whole dataset in PCA\n",
    "variance1, var1 = PCA_info_notransform(final_X_norm, final_features)\n",
    "n_comp = PCA_plot(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is shown above, from index 17 on, the features are sufficient to retrieve above 90% of the variance within the datasets. Therefore it is chosen to perform a PCA analysis with the number of 18 components. A new dataset will be created with 18 components, that are based on the old 40 important features that were retrieved in the features selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the new X_pca dataset\n",
    "pca, X_pca, variance, var = PCA_transform(final_X_norm, n_comp)\n",
    "df_explained_variance, mean_explained_variance, idx = PCA_transform_overview(variance, var)\n",
    "# print(df_explained_variance.shape)\n",
    "# scree_plot(df_explained_variance, mean_explained_variance, idx) #can be done for every component, now only component 1\n",
    "# factor_loadings(X_tr_norm_selected, pca, df_explained_variance) #heatmap should be improved after better features selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap above shows for every new component which features are used to create this new component.\n",
    "NOTE: get this more insightful; how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "It is time to find a model that can make predictions whether a tumor is benign or malign.\n",
    "There are a lot of different ways of classifying. However we have only so much time and not every classifier makes sense.\n",
    "To get a quick overview of which classifiers might be helpful for this problem, some general classifiers have been used on the current training data.\n",
    "After this general research, a few classifiers will be chosen to do tweak the hyperparameters with crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of classifiers\n",
    "# clsfs = [LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), GaussianNB(),\n",
    "#          LogisticRegression(), SGDClassifier(), KNeighborsClassifier(), \n",
    "#          RandomForestClassifier(), DecisionTreeClassifier(), svm.SVC()]\n",
    "# clsfs_strings = [str(c) for c in clsfs]\n",
    "\n",
    "\n",
    "# y_tr1_bin = y_tr1['label'].replace({'benign': 0, 'malignant': 1})\n",
    "# y_tr1_bin = y_tr1_bin.to_numpy()\n",
    "\n",
    "# f1_scores = classifiers(clsfs, X_pca, y_tr1_bin)\n",
    "# print(f\"list of the f1-scores {f1_scores}\")\n",
    "\n",
    "# # Create an empty dataframe with the desired column name\n",
    "# df_f1_scores = pd.DataFrame(columns=['f1-score'], index=clsfs_strings)\n",
    "\n",
    "# # Add the f1-scores for each classifier as a row in the dataframe\n",
    "# for clsfs_str, score in zip(clsfs_strings, f1_scores):\n",
    "#     df_f1_scores.loc[clsfs_str] = score\n",
    "\n",
    "# df_f1_scores = df_f1_scores[df_f1_scores['f1-score'] != 1.0] # overfitting is not good so get rid of the 1.0    \n",
    "# print(df_f1_scores)\n",
    "# top_k = 5\n",
    "# df_f1_scores.sort_values(by='f1-score', ascending=False)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(X_train, X_test, y_train, y_test, n_iter=30):\n",
    "    best_f1_RF = 0\n",
    "    best_params_RF = {}\n",
    "    f1_scores_rf = []\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(5,70),\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    }\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        rf = RandomForestClassifier(random_state=i)\n",
    "        rs = RandomizedSearchCV(\n",
    "            rf,\n",
    "            param_distributions=param_dist,\n",
    "            cv=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        rs.fit(X_train, y_train)\n",
    "        best_rf = rs.best_estimator_\n",
    "        y_pred = best_rf.predict(X_test)\n",
    "\n",
    "        F1 = metrics.f1_score(y_test, y_pred)\n",
    "        f1_scores_rf.append(F1)\n",
    "\n",
    "        if F1 > best_f1_RF:\n",
    "            best_f1_RF = F1\n",
    "            best_params_RF = rs.best_params_\n",
    "\n",
    "    print('Best hyperparameters:', best_params_RF)\n",
    "    print('Best F1 score:', best_f1_RF)\n",
    "    return best_params_RF, best_f1_RF\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_d_bin, test_size=0.2, random_state=42)\n",
    "\n",
    "best_params_RF, best_f1_RF = RF(X_train, X_test, y_train, y_test)\n",
    "\n",
    "RF_best_class = RandomForestClassifier(**best_params_RF)\n",
    "scores_rf = cross_val_score(RF_best_class, X_pca, y_d_bin, cv=10, scoring='f1_macro')\n",
    "print(\"Random forrest plot: the cross validation has a F1-score of %0.2f with a standard deviation of %0.2f\" % (scores_rf.mean(), scores_rf.std()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After several runs: \n",
    "Best hyperparameters: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 6, 'n_estimators': 20}\\\n",
    "Best F1 score: 0.9600000000000001\\\n",
    "Random forrest plot: the cross validation has a F1-score of 0.81 with a standard deviation of 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train, X_test, y_train, y_test, n_iter=50):\n",
    "    best_f1_svm = 0\n",
    "    best_params_svm = {}\n",
    "    f1_scores_svm = []\n",
    "    param_dist_svm = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 7))\n",
    "    }\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        rf_svm = SVC(random_state=i)\n",
    "        rs_svm = RandomizedSearchCV(\n",
    "            rf_svm,\n",
    "            param_distributions=param_dist_svm,\n",
    "            cv=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        rs_svm.fit(X_train, y_train)\n",
    "        best_rf_svm = rs_svm.best_estimator_\n",
    "        y_pred_svm = best_rf_svm.predict(X_test)\n",
    "\n",
    "        F1_svm = metrics.f1_score(y_test, y_pred_svm)\n",
    "        f1_scores_svm.append(F1_svm)\n",
    "\n",
    "        if F1_svm > best_f1_svm:\n",
    "            best_f1_svm = F1_svm\n",
    "            best_params_svm = rs_svm.best_params_\n",
    "\n",
    "    print('Best hyperparameters:', best_params_svm)\n",
    "    print('Best F1 score:', best_f1_svm)\n",
    "    return best_params_svm, best_f1_svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_d_bin, test_size=0.2, random_state=42)\n",
    "best_params_svm, best_f1_svm = svm(X_train, X_test, y_train, y_test)\n",
    "\n",
    "svm_best_class = SVC(**best_params_svm)\n",
    "scores_svm = cross_val_score(svm_best_class, X_pca, y_d_bin, cv=10, scoring='f1_macro')\n",
    "print(\"Support Vector Machine: the cross validation has a F1-score of %0.2f with a standard deviation of %0.2f\" % (scores_svm.mean(), scores_svm.std()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected hyperparameters for SVM as input ensembling classifier in final model\n",
    "Best hyperparameters: {'kernel': 'rbf', 'gamma': 0.001, 'C': 10}\\\n",
    "Best F1 score: 0.888888888888889\\\n",
    "Support Vector Machine: the cross validation has a F1-score of 0.88 with a standard deviation of 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(X_train, X_test, y_train, y_test, n_iter=6):\n",
    "    best_f1_lda = 0\n",
    "    best_params_lda = {}\n",
    "    f1_scores_lda = []\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    param_dist_lda = {\n",
    "        'store_covariance': [True, False],\n",
    "        'solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'priors': [None] + [list(np.random.dirichlet(np.ones(n_classes))) for i in range(10)],\n",
    "    }\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        rf_lda = LinearDiscriminantAnalysis()\n",
    "        rs_lda = RandomizedSearchCV(\n",
    "            rf_lda,\n",
    "            param_distributions=param_dist_lda,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=i\n",
    "        )\n",
    "\n",
    "        rs_lda.fit(X_train, y_train)\n",
    "        best_rf_lda = rs_lda.best_estimator_\n",
    "        y_pred_lda = best_rf_lda.predict(X_test)\n",
    "\n",
    "        F1_lda = metrics.f1_score(y_test, y_pred_lda)\n",
    "        f1_scores_lda.append(F1_lda)\n",
    "\n",
    "        if F1_lda > best_f1_lda:\n",
    "            best_f1_lda = F1_lda\n",
    "            best_params_lda = rs_lda.best_params_\n",
    "\n",
    "    print('Best hyperparameters:', best_params_lda)\n",
    "    print('Best F1 score:', best_f1_lda)\n",
    "    return best_params_lda, best_f1_lda\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_d_bin, test_size=0.2, random_state=42)\n",
    "best_params_lda, best_f1_lda = lda(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "lda_best_class = LinearDiscriminantAnalysis(**best_params_lda)\n",
    "scores_lda = cross_val_score(lda_best_class, X_pca, y_d_bin, cv=10, scoring='f1_macro')\n",
    "print(\"Linear Discriminant Analysis: the cross validation has a F1-score of %0.2f with a standard deviation of %0.2f\" % (scores_lda.mean(), scores_lda.std()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected hyperparameters for LDA as input ensembling classifier in final model\n",
    "Best hyperparameters: {'store_covariance': True, 'solver': 'eigen', 'priors': [0.5287484663893824, 0.4712515336106177]}\\\n",
    "Best F1 score: 0.7857142857142856\\\n",
    "Linear Discriminant Analysis: the cross validation has a F1-score of 0.91 with a standard deviation of 0.14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "Next ensemble will be used to look if the several classifiers can compromise for each weaknesses and then in the end get a better result. Ensembling is a whole topic on its own, and therefore it is choosen to only do one ensemble. The three classifiers with the best hyperparameters will be used to create one classifiers that uses them all. By cross validation this new classifier will be compared to each one on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble of bagged classifiers\n",
    "svm_bag = BaggingClassifier(estimator=svm_best_class, n_estimators=10, random_state=42)\n",
    "lda_bag = BaggingClassifier(estimator=lda_best_class, n_estimators=10, random_state=42)\n",
    "rf_bag = BaggingClassifier(estimator=RF_best_class, n_estimators=10, random_state=42)\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[('svm', svm_bag), ('lda', lda_bag), ('rf', rf_bag)], voting='hard')\n",
    "\n",
    "# Perform cross-validation on the ensemble model\n",
    "scores_ensemble = cross_val_score(ensemble, X_pca, y_d_bin, cv=10, scoring='f1_macro')\n",
    "\n",
    "# Print the mean and standard deviation of the F1 scores\n",
    "print(\"Ensemble: the cross-validation has a mean F1 score of %0.2f with a standard deviation of %0.2f\" % (scores_ensemble.mean(), scores_ensemble.std()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "Ft_s = load_Ft_set()\n",
    "D_s = load_D_set()\n",
    "\n",
    "X_ft, y_ft = return_X_y(Ft_s)\n",
    "X_d, y_d = return_X_y(D_s)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_d_bin = le.fit_transform(y_d.to_numpy().reshape(-1,))\n",
    "y_ft_bin = le.fit_transform(y_ft.to_numpy().reshape(-1,))\n",
    "\n",
    "# Parameters needed: removed_features_preprocess, final_features, n_comp, **rf_hyperparams, **svm_hyperparams, **lda_hyperparams\n",
    "X_filtered, removed_features_preprocess = preprocessing(X_d)\n",
    "final_features\n",
    "\n",
    "rf_hyperparams = {\n",
    "    'max_depth': 20,\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 6,\n",
    "    'n_estimators': 20\n",
    "}\n",
    "\n",
    "svm_hyperparams = {\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 0.001,\n",
    "    'C': 10\n",
    "}\n",
    "\n",
    "lda_hyperparams = {\n",
    "    'store_covariance': False,\n",
    "    'solver': 'svd',\n",
    "    'priors': [0.6231810575673354, 0.3768189424326645]\n",
    "}\n",
    "\n",
    "\n",
    "def remove_features(X, removed_features_preprocess):\n",
    "    # implementation of feature removal function\n",
    "    return X.drop(columns=removed_features_preprocess)\n",
    "\n",
    "\n",
    "def outliers(X):\n",
    "    X_clean = X.copy()\n",
    "\n",
    "    means = X.mean()\n",
    "    stds = X.std()\n",
    "    threshold = 3\n",
    "    for feature in X.columns:\n",
    "\n",
    "        # Calculate the lower and upper bounds for the feature\n",
    "        lower_bound = means[feature] - (stds[feature] * threshold)\n",
    "        upper_bound = means[feature] + (stds[feature] * threshold)\n",
    "\n",
    "        # Replace outliers with upper and lower bound\n",
    "        X_clean.loc[X_clean[feature] < lower_bound, feature] = lower_bound\n",
    "        X_clean.loc[X_clean[feature] > upper_bound, feature] = upper_bound\n",
    "    \n",
    "    return X_clean\n",
    "\n",
    "\n",
    "def normalize_standard(X):\n",
    "    # apply StandardScaler to each column of X\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # create a new DataFrame with scaled data\n",
    "    X_normalized = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    return X_normalized\n",
    "\n",
    "\n",
    "def feature_selection(X, final_features):\n",
    "    return X[final_features]\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('remove_features', FunctionTransformer(remove_features, kw_args={'removed_features_preprocess': removed_features_preprocess})),\n",
    "    ('outliers', FunctionTransformer(outliers)),\n",
    "    ('normalize', FunctionTransformer(normalize_standard)),\n",
    "    ('feature_selection', FunctionTransformer(feature_selection, kw_args={'final_features': final_features})),\n",
    "    ('pca', PCA(n_components=n_comp)),\n",
    "    ('bagging', BaggingClassifier(\n",
    "        estimator=VotingClassifier([\n",
    "            ('rf', RandomForestClassifier(**rf_hyperparams)),\n",
    "            ('svm', SVC(**svm_hyperparams)),\n",
    "            ('lda', LinearDiscriminantAnalysis(**lda_hyperparams))\n",
    "        ], voting='hard'),\n",
    "        n_estimators=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_d, y_d_bin)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred = pipeline.predict(X_ft)\n",
    "\n",
    "f1_macro = f1_score(y_ft_bin, y_pred, average='macro')\n",
    "print('The F1 score of the pipeline on the test set is:', f1_macro)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce824650e4ae37724812ef9e49660f21f90a20c225745616737e898b15b66eb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
