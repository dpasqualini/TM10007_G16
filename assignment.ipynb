{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run this to use from colab environment\n",
    "%pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "from worcliver.load_data import load_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean-up (before splitting data)\n",
    "In this section double entries, such as repeated features and samples, will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples (after removal of duplicates): 186, the number of duplicates that had to be removed: 0\n",
      "The number of columns (after removal of duplicates): 494, the number of duplicates that had to be removed: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "'''\n",
    "These 2 lines add a duplicate sample in the DataFrame, you can than observe what the effect is \n",
    "in the printed sentence. If you'd like to see it indeed works, please uncomment these lines.\n",
    "'''\n",
    "# row = df.iloc[0].copy()\n",
    "# df = df.append(row, ignore_index=True)\n",
    "'''\n",
    "In the lines below we first drop all duplicate entries, whether it is a duplicate row or a column. \n",
    "Additionally, the number of duplicates will be returned to the user.\n",
    "'''\n",
    "\n",
    "df_f = df.drop_duplicates()\n",
    "print(f'The number of samples (after removal of duplicates): {len(df_f.index)}, the number of duplicates that had to be removed: {len(df.index)-len(df_f.index)}')\n",
    "print(f'The number of columns (after removal of duplicates): {len(df_f.columns)}, the number of duplicates that had to be removed: {len(df.columns)-len(df_f.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_f                                       # So we can just use df from now on\n",
    "df = df.replace('', np.nan)                     # Any empty values are replaced for NaN\n",
    "# df.describe(include='all')                    # Uncomment if interested"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The table above shows there are only 2 labels present in the dataset, from observation of the csv.file we can see the labels are 'malignant' and 'benigne'. What is also stated here, that 94 of the 186 samples was labeled malignant (and thus 92 samples are labeled benigne.) This information is usefull in the next step, to determine whether a stratified splitting of our data, would be necessary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting (1/2)\n",
    "In this section the dataset will be split into two datasets, the Design set (D_set) and the Final Test set (Ft_set). This splitting will be performed randomly where 80% will end up in D_set and the remaining 20% will be stored in Ft_set. Ft_set will not be used at all, until the very latest to evaluate our tool and establish how generalising the tool is. In the initial split we will randomly split the dataset, as both labels are equally abundant in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# D_set, Ft_set = train_test_split(df, test_size=0.2)\n",
    "# D_set = pd.DataFrame(D_set)\n",
    "# Ft_set = pd.DataFrame(Ft_set)\n",
    "\n",
    "# D_set.describe(include='all')                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThese lines were used to save the splitted sets, to make sure Design and Test set won't alter.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D_set.to_csv(\"C:/Users/dgjpa/D_set.csv\")\n",
    "# Ft_set = Ft_set.to_csv(\"C:/Users/dgjpa/Ft_set.csv\")\n",
    "\n",
    "'''\n",
    "These lines were used to save the splitted sets, to make sure Design and Test set won't alter.\n",
    "''' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting (2/2)\n",
    "In this section D_set will be split into two datasets, the train set (Tr_set) and the Validation set (Va_set). This splitting will be performed randomly where 90% will end up in Tr_set and the remaining 10% will be stored in Va_set. This second splitting will be repeated 10 times with each iteration a different 10% of the samples will be put into the Va_set, as we will use cross-validation to make most out of the data available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load D_set here and start second split."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
